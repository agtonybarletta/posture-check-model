{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6aafa2-cf58-42fe-abe0-1f9fc3c92390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.preprocessing import image\n",
    "import keras.utils as image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3905a7e2-226c-4cf1-93ab-c4202f8d4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMENSIONS AND CONSTANT\n",
    "\n",
    "SIZE_X = 224\n",
    "SIZE_Y = 224\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1037f8e-d44d-460c-b6d5-f9e65ad63542",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile = keras.applications.mobilenet.MobileNet()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4039dac7-aded-4852-80cb-50bfbfddef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=MobileNet(weights='imagenet',include_top=False, input_shape=(224, 224, 3)) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(3,activation='softmax')(x) #final layer with softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbc7b26-5696-4378-bdf9-c025b77f84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "#specify the inputs\n",
    "#specify the outputs\n",
    "#now a model has been created based on our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8df1a1-a978-496a-b327-507d3447738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "# or if we want to set the first 20 layers of the network to be non-trainable\n",
    "for layer in model.layers[:20]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[20:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e460c1-70c0-492e-9b2e-a3cc6a7d60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,layer in enumerate(model.layers):\n",
    "#  print(i,layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202864a6-e631-45a5-a2b2-38c4688de6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "\n",
    "#train_generator=train_datagen.flow_from_directory('/home/agtonybarletta/projects/posture_check/data',\n",
    "#                                                 target_size=(224,224),\n",
    "#                                                 color_mode='rgb',\n",
    "#                                                 batch_size=BATCH_SIZE,\n",
    "#                                                 class_mode='categorical',\n",
    "#                                                 shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd827fe1-c085-46dc-8a72-2b2e4aede381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174 files belonging to 3 classes.\n",
      "Using 34 files for validation.\n",
      "Found 174 files belonging to 3 classes.\n",
      "Using 140 files for training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory='/home/agtonybarletta/projects/posture_check/data',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1,\n",
    "    shuffle = True\n",
    ")\n",
    "validation_ds\n",
    "training_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory='/home/agtonybarletta/projects/posture_check/data',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1,\n",
    "    shuffle = True\n",
    ")\n",
    "training_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4ba7e82-0339-4306-9997-29621c8df285",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m*\u001b[39mBATCH_SIZE\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of samples total:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples)\n\u001b[0;32m---> 19\u001b[0m print_ds_info(\u001b[43mtraining_ds\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_ds' is not defined"
     ]
    }
   ],
   "source": [
    "def print_ds_info(dataset):\n",
    "    # Create a tf.data.Dataset object\n",
    "    \n",
    "    # Retrieve shape information using element_spec\n",
    "    input_shape = dataset.element_spec[0].shape\n",
    "    output_shape = dataset.element_spec[1].shape\n",
    "    \n",
    "    # Print the shape information\n",
    "    print(\"Input shape:\", input_shape)\n",
    "    print(\"Output shape:\", output_shape)\n",
    "    \n",
    "    # Get the number of samples\n",
    "    num_samples = len(dataset)\n",
    "    print(\"Number of samples batched:\", num_samples)\n",
    "\n",
    "    # Get the number of samples\n",
    "    num_samples = len(dataset)*BATCH_SIZE\n",
    "    print(\"Number of samples total:\", num_samples)\n",
    "print_ds_info(training_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "906d7f3f-fb08-45b5-a1dd-a13ca08e09a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# show example data\\n\\n\\n# Data Pipeline\\ndef convert_to_float(image, label):\\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\\n    return image, label\\n\\nAUTOTUNE = tf.data.experimental.AUTOTUNE\\nds_train = (\\n    training_ds\\n    .map(convert_to_float)\\n    .cache()\\n    .prefetch(buffer_size=AUTOTUNE)\\n)\\n\\nplt.figure(figsize=(10,10))\\ntraining_ds_generator_f = iter(training_ds.unbatch().map(lambda x, y: x/224).batch(1))\\nex = next(training_ds_generator_f)\\nfor i in range(16):\\n    image = ex\\n    plt.subplot(4, 4, i+1)\\n    plt.imshow(tf.squeeze(image))\\n    plt.axis('off')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# show example data\n",
    "\n",
    "\n",
    "# Data Pipeline\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = (\n",
    "    training_ds\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "training_ds_generator_f = iter(training_ds.unbatch().map(lambda x, y: x/224).batch(1))\n",
    "ex = next(training_ds_generator_f)\n",
    "for i in range(16):\n",
    "    image = ex\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(tf.squeeze(image))\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4848802b-5652-4603-9459-d8c3a5dc48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(ds):\n",
    "    samples = ds.unbatch().map(lambda x, y: x/224).batch(1)\n",
    "    gen = iter(samples)\n",
    "    for i in range(min(len(list(samples)), 16)):\n",
    "        image = next(gen)\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(tf.squeeze(image))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c1c0897-39c9-40a2-964d-3a9130c1c477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, 224, 224, 3)\n",
      "Output shape: (None, 3)\n",
      "Number of samples batched: 35\n",
      "Number of samples total: 140\n",
      "Input shape: (None, 224, 224, 3)\n",
      "Output shape: (None, 3)\n",
      "Number of samples batched: 35\n",
      "Number of samples total: 140\n",
      "Input shape: (None, 224, 224, 3)\n",
      "Output shape: (None, 3)\n",
      "Number of samples batched: 140\n",
      "Number of samples total: 560\n"
     ]
    }
   ],
   "source": [
    "#augment\n",
    "augment = keras.Sequential([\n",
    "    preprocessing.RandomContrast(factor=0.5),\n",
    "    preprocessing.RandomFlip(mode='horizontal'), # meaning, left-to-right\n",
    "    # preprocessing.RandomFlip(mode='vertical'), # meaning, top-to-bottom\n",
    "    # preprocessing.RandomWidth(factor=0.15), # horizontal stretch\n",
    "    preprocessing.RandomRotation(factor=0.05),\n",
    "    preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "])\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "augmented_training_ds= training_ds.map(lambda x, y: (augment(x), y))\n",
    "\n",
    "new_training_ds = training_ds \\\n",
    ".concatenate(augmented_training_ds) \\\n",
    ".concatenate(augmented_training_ds) \\\n",
    ".concatenate(augmented_training_ds)\n",
    "print_ds_info(training_ds)\n",
    "print_ds_info(augmented_training_ds)\n",
    "print_ds_info(new_training_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e96c643f-1dc1-41a7-a232-06d4a71568ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsmall_ds = training_ds.take(1)\\nprint_ds_info(training_ds)\\nsmall_aug_ds = small_ds.map(lambda x, y: (augment(x), y))\\nprint_ds_info(small_aug_ds)\\n\\nnew_small_ds = small_ds.concatenate(small_aug_ds).concatenate(small_aug_ds).concatenate(small_aug_ds)\\n\\nshow_sample(small_ds)\\nshow_sample(small_aug_ds)\\nshow_sample(new_small_ds)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "small_ds = training_ds.take(1)\n",
    "print_ds_info(training_ds)\n",
    "small_aug_ds = small_ds.map(lambda x, y: (augment(x), y))\n",
    "print_ds_info(small_aug_ds)\n",
    "\n",
    "new_small_ds = small_ds.concatenate(small_aug_ds).concatenate(small_aug_ds).concatenate(small_aug_ds)\n",
    "\n",
    "show_sample(small_ds)\n",
    "show_sample(small_aug_ds)\n",
    "show_sample(new_small_ds)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4194b34a-312b-4e68-956d-9d2fb62f421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['correct', 'incorrect', 'none']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 19:37:49.980977: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [140]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-08 19:37:49.981737: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [140]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'correct' has 12 samples.\n",
      "Class 'incorrect' has 18 samples.\n",
      "Class 'none' has 5 samples.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store the count for each class\n",
    "class_names = training_ds.class_names\n",
    "print(class_names)\n",
    "class_count = {class_name: 0 for class_name in class_names}\n",
    "\n",
    "# Iterate over the dataset and count the samples for each class\n",
    "for _, label in training_ds:\n",
    "    label_index = np.argmax(label.numpy())  # Convert one-hot encoded label to class index\n",
    "    class_name = class_names[label_index]  # Get the class name corresponding to the class index\n",
    "    class_count[class_name] += 1\n",
    "\n",
    "# Print the count for each class\n",
    "for class_name, count in class_count.items():\n",
    "    print(f\"Class '{class_name}' has {count} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b02ebd5d-c6b8-4710-a5ab-4eae1e7ed3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/140 [..............................] - ETA: 11:19 - loss: 2.2430e-04 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#model.fit_generator(generator=train_generator,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#                   steps_per_epoch=step_size_train,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#                   epochs=25)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m class_weight \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     12\u001b[0m                 \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     13\u001b[0m                 \u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m3\u001b[39m}\n\u001b[0;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_training_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_checkpoint-shuffle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_checkpoint-shuffle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/projects/posture_check/jupyterlab/venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "\n",
    "step_size_train=len(new_training_ds)\n",
    "step_size_train\n",
    "#model.fit_generator(generator=train_generator,\n",
    "#                   steps_per_epoch=step_size_train,\n",
    "#                   epochs=25)\n",
    "class_weight = {0: 2,\n",
    "                1: 1,\n",
    "                2: 3}\n",
    "history = model.fit(new_training_ds, validation_data=validation_ds, steps_per_epoch=step_size_train,\n",
    "                  epochs=50, class_weight=class_weigth)\n",
    "model.save_weights('my_checkpoint-shuffle2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "450d8d6d-0574-4173-80ff-2e88a7c5a626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f34bdf0ce10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('my_checkpoint-shuffle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4302aadf-75f8-4422-a722-cbb2b107866c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7180e4d6-a6ff-40c6-b372-d3f6b653d7a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(probability)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 11\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_ds\u001b[49m\u001b[38;5;241m.\u001b[39mclass_names\n\u001b[1;32m     13\u001b[0m print_class_probabilities(pred, class_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_ds' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def print_class_probabilities(y_pred, class_labels):\n",
    "    n_samples, n_classes = y_pred.shape\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        probabilities = y_pred[i]\n",
    "        for class_idx, probability in enumerate(probabilities):\n",
    "            class_label = class_labels[class_idx]\n",
    "            print(f' {class_label}: {\"{:.3f}\".format(probability)}', end='')\n",
    "        print()\n",
    "\n",
    "class_labels = training_ds.class_names\n",
    "\n",
    "print_class_probabilities(pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e025034-930a-4cdd-b823-63ce57b285b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct\n",
    "#img_path = '/home/agtonybarletta/projects/posture_check/data/correct/opencv_frame_2023-07-07T13:05:22.608620.png'\n",
    "#img_path = '/home/agtonybarletta/projects/posture-check-model/data/correct/opencv_frame_2023-07-07T13:05:22.608620.png'\n",
    "#img_path = '/home/agtonybarletta/projects/posture_check/data/correct/opencv_frame_2023-03-06T12:26:08.022630.png'\n",
    "#incorrect\n",
    "img_path = '/home/agtonybarletta/projects/posture-check-model/data/correct/opencv_frame_2023-03-06T12:26:08.022630.png'\n",
    "#img_path = '/home/agtonybarletta/projects/posture_check/data/incorrect/opencv_frame_2023-07-05T15:12:43.895980.png'\n",
    "#img_path = '/home/agtonybarletta/projects/posture_check/data/incorrect/opencv_frame_2023-03-03T14:32:16.381431.png'\n",
    "#Image(filename=img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d46f0521-49d9-44e7-9e06-8ce1c7f5d56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.6258707e-02, 9.7373331e-01, 8.0610871e-06]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "    #img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return img_tensor\n",
    "\n",
    "new_image = load_image(img_path)\n",
    "\n",
    "pred = model.predict(new_image)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7444c900-d31a-40d9-ad0b-2e63b2e3f101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      " correct: 0.000 incorrect: 1.000 none: 0.000\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      " correct: 1.000 incorrect: 0.000 none: 0.000\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      " correct: 0.998 incorrect: 0.002 none: 0.000\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      " correct: 0.103 incorrect: 0.897 none: 0.000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 0.226 incorrect: 0.774 none: 0.000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 0.000 incorrect: 0.001 none: 0.999\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 0.000 incorrect: 0.000 none: 1.000\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      " correct: 0.993 incorrect: 0.007 none: 0.000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 1.000 incorrect: 0.000 none: 0.000\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      " correct: 0.008 incorrect: 0.992 none: 0.000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 0.004 incorrect: 0.996 none: 0.000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 0.007 incorrect: 0.993 none: 0.000\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      " correct: 0.001 incorrect: 0.999 none: 0.000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 0.999 incorrect: 0.001 none: 0.000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 0.058 incorrect: 0.942 none: 0.000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " correct: 1.000 incorrect: 0.000 none: 0.000\n",
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"test\")\n",
    "\n",
    "model.load_weights('my_checkpoint-shuffle')\n",
    "\n",
    "def prepare_image(file):\n",
    "    img_path = ''\n",
    "    img = image.load_img(img_path + file, target_size=(224, 224))\n",
    "    #img_array = image.img_to_array(img)\n",
    "    #img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "    return img\n",
    "  \n",
    "\n",
    "img_counter = 0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    cv2.imshow(\"test\", frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "    if time.time() - start_time >= 3:\n",
    "        start_time = time.time()\n",
    "        img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        # print(\"{} written!\".format(img_name))\n",
    "        preprocessed_image = load_image(img_name)\n",
    "        predictions = model.predict(preprocessed_image)\n",
    "        \n",
    "        #results = imagenet_utils.decode_predictions(predictions)\n",
    "        print_class_probabilities(predictions, class_labels)\n",
    "cam.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9c64c81-982b-422f-965c-7a6e41e38fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 19:34:37.180367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [34]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-07-08 19:34:37.180860: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [34]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 1s - loss: 4.8719 - accuracy: 0.0000e+00 - 915ms/epoch - 102ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(validation_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55f3e7-e6e0-4e00-92ed-4ba1226d0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_ds, steps_per_epoch=step_size_train,\n",
    "                  epochs=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ba1c2-2f42-4bd6-984b-0bc909094445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
